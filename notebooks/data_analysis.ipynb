{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "# test = pd.read_csv(\"../data/test.csv\")\n",
    "train, test = train_test_split(train, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>can you check whether its cancelled completely?</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cannot rely on both milk delivery and grocery ...</td>\n",
       "      <td>Milk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I get no notification, however the app is real...</td>\n",
       "      <td>notification</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Love this app, but would love it even more if ...</td>\n",
       "      <td>view</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it does not let me load a clip on the scene</td>\n",
       "      <td>load</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        aspect  label\n",
       "0    can you check whether its cancelled completely?     cancelled      1\n",
       "1  cannot rely on both milk delivery and grocery ...          Milk      0\n",
       "2  I get no notification, however the app is real...  notification      0\n",
       "3  Love this app, but would love it even more if ...          view      1\n",
       "4        it does not let me load a clip on the scene          load      0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plt = train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df['text'] = df['text'].apply(lambda x : str(x).lower())\n",
    "    df['label'] = df['label'].apply(lambda x : int(x))\n",
    "    df['aspect'] = df['aspect'].apply(lambda x : str(x).lower())\n",
    "    df['text'] = df['text'].apply(lambda x : str(x).translate(str.maketrans('','',string.punctuation))) #removing punctuation\n",
    "    df['aspect'] = df['aspect'].apply(lambda x : str(x).translate(str.maketrans('','',string.punctuation))) #removing punctuation\n",
    "    df['text'] = df['text'].apply(lambda x : str(x).translate(str.maketrans('','',string.digits))) #removing integers\n",
    "    df['aspect'] = df['aspect'].apply(lambda x : str(x).translate(str.maketrans('','',string.digits))) #removing integers\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1680 1294 1026]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "names = ['negative', 'neutral', 'positive']\n",
    "# print(dir(data_plt))\n",
    "print(data_plt.values)\n",
    "vals = list(data_plt.values)\n",
    "plt.bar(names,vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1680\n",
      "1    1294\n",
      "2    1026\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data_plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    train_f1 = preprocess(train)\n",
    "    test_f1 = preprocess(test)\n",
    "    test_f1 = preprocess(test)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    datasets = {\n",
    "        \"train\": train_f1,\n",
    "        \"test\": test_f1,\n",
    "    }\n",
    "    for i in datasets:\n",
    "        data = datasets[i] # Get the dataset\n",
    "        labels = list(data.label)\n",
    "        encoded_tokenize = tokenizer(list(data.text),\n",
    "         list(data.aspect), \n",
    "         padding=True, \n",
    "         truncation=True, \n",
    "         max_length = 512, \n",
    "         return_tensors = \"pt\")\n",
    "        datasets[i] = torch.utils.data.TensorDataset(\n",
    "            encoded_tokenize[\"input_ids\"],\n",
    "            encoded_tokenize[\"attention_mask\"],\n",
    "            torch.tensor(labels, dtype=torch.long).reshape(-1,1)\n",
    "        )\n",
    "    \n",
    "    return datasets[\"train\"], datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(confusion: np.array) -> float:\n",
    "    return confusion.trace() / confusion.sum().sum()\n",
    "\n",
    "\n",
    "def mcc(confusion: np.array) -> float:\n",
    "    t = confusion.sum(0)\n",
    "    p = confusion.sum(1)\n",
    "    c = confusion.trace()\n",
    "    s = confusion.sum().sum()\n",
    "\n",
    "    num = c*s - t.dot(p)\n",
    "    den = np.sqrt(s**2 - p.dot(p))*np.sqrt(s**2 - t.dot(t))\n",
    "\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_absa(config: dict):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    num_labels = 3\n",
    "    numeric_labels = list(range(num_labels))\n",
    "\n",
    "    model = DistilBertModel.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config[\"lr\"]\n",
    "    )\n",
    "\n",
    "    train_set, test_set = load()\n",
    "\n",
    "    cutoff = int(0.8 * len(train_set))\n",
    "    train_subset, val_subset = random_split(train_set, [cutoff, len(train_set) - cutoff])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=int(config[\"batch_size\"]),\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    total = (cutoff // config[\"batch_size\"]) + int(cutoff % config[\"batch_size\"] != 0)\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "\n",
    "        for i, (input_ids, attention_mask, labels) in tqdm(enumerate(train_loader), total=total):\n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(\n",
    "                input_ids=input_ids.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "                labels=labels.to(device)\n",
    "            )\n",
    "            loss = output[\"loss\"]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        confusion = np.zeros([num_labels, num_labels])\n",
    "\n",
    "        for i, (input_ids, attention_mask, labels) in enumerate(val_loader):\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                output = model(\n",
    "                    input_ids=input_ids.to(device),\n",
    "                    attention_mask=attention_mask.to(device),\n",
    "                    labels=labels.to(device)\n",
    "                )\n",
    "                loss, logits = output[\"loss\"], output[\"logits\"]\n",
    "\n",
    "                confusion += confusion_matrix(\n",
    "                    y_true=labels.flatten().cpu(), \n",
    "                    y_pred=torch.softmax(logits, -1).argmax(-1).cpu(), \n",
    "                    labels=numeric_labels\n",
    "                )\n",
    "\n",
    "                running_loss += loss.cpu().numpy()\n",
    "                steps += 1\n",
    "\n",
    "        summary = dict(\n",
    "            loss=(running_loss / steps),\n",
    "            accuracy=acc(confusion),\n",
    "            mcc=mcc(confusion)\n",
    "        )\n",
    "        print(summary)\n",
    "\n",
    "\n",
    "    torch.save(model, f\"./absa-distil_bert.pt\")\n",
    "    print(\"Finished training.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\n",
    "    \"epochs\": 4,\n",
    "    \"batch_size\": 24,\n",
    "    \"lr\": 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 268M/268M [00:23<00:00, 11.3MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21672/2011896282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_absa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_21672/1982172979.py\u001b[0m in \u001b[0;36mtrain_absa\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'distilbert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     optimizer = AdamW(\n",
      "\u001b[0;32m/media/sanchit/New1/enterpret/enterpret-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/media/sanchit/New1/enterpret/enterpret-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sanchit/New1/enterpret/enterpret-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sanchit/New1/enterpret/enterpret-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sanchit/New1/enterpret/enterpret-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "model = train_absa(config)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "769a8819f3e3355fd3b9cf0b9760837342d539595e65842cf2f42ff50a75406d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('enterpret-env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}